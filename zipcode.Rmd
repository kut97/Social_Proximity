---
title: "Social Proximity Zipcode"
author: 
date: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
To do:

***add maps*** - ask Kushagra **connect if sci > threshold** plot map with thickness of lines being the sci (thicker = higher sci)
***provide connectivity for census tract*** - Zitao and Yanrong **produce sci for census tract** **weighting by proportion of zipcode in a census tract**
**create meta data** - Erin
** put in jupyter notebook**
**combine rural/urban with this analysis**
**clean it up**

Social Networks can be a significant factor in understanding the dynamics of various aspects of different fields such as social,economical and public health. In order to substantiate the intensity of social network between individuals Bailey et al proposed a relative frequency of Friendship links between different geographical unit called as Social Connectedness Index. Due to the nature of its relative frequency a comparison between the intensity of social ties across different locations can now be possible. Further Kulcher et al proposed another measure understanding the social influence of alters on ego called social proximity. This markdown file is a guide for other researchers who would want to make use of Social Proximity in there research.

```{r}
library(tidyverse)
library(igraph)
library(glmnet)
library(pls)
library(leaps)
library(class)
library(tree)
library (randomForest)
```

We create the social proximity values for each zipcode beginning with a 1. We then drop zipcodes that fall outside of Pennsylvania. As a first step we want to convert the SCI values into relative probabilities so we divide the SCI by the highest frequency of SCI and create a data frame with user location fr_locations and their probability values.

```{r}
df_0 <- read_tsv ('zcta_zcta_shard1.tsv')
df_0 <- df_0 %>% dplyr::mutate(probabilites=scaled_sci/(1000000000)) 
df_1 <- df_0 %>% dplyr::filter(user_loc %in% 15001:19612 & fr_loc %in% 15001:19612)
df_1 <- unique(df_1)
df_1 <- df_1 %>% distinct(probabilites,.keep_all = TRUE) ##single repeated pairwise comparisons for sci
user_loc <- df_1 %>% distinct(user_loc)
```

```{r}
library(zipcodeR)
df_1 <- df_1 %>% mutate('distance' = zip_distance(df_1$user_loc, df_1$fr_loc))

```


Generating weights for  1256 zipcodes and merging it in a data frame.
```{r}
dim(user_loc)
fr_loc <- df_1 %>% distinct(fr_loc)
wt <- rnorm(1256,0.13,1)
q_i <- data.frame(fr_loc,wt)
```

Further to create a weighted probabilities scaled by SCI and the weights of friend location we create a new data frame and mutate it with weighted SCI. 
```{r}
df_s <- merge(df_1,q_i,by="fr_loc")
df_s <- df_s %>% mutate(wt_sci=probabilites*wt) %>% filter(scaled_sci != 1)
```
Now let's do some plotting of distance with wt_sci. 
```{r}
plot(df_s$distance$distance, df_s$wt_sci)
plot(df_s$distance$distance, df_s$wt_sci, xlim = c(0, 30))
```
```{r}
# Download USA Income Tax Data by Zip code here: https://www.kaggle.com/datasets/wpncrh/zip-code-income-tax-data-2014?resource=download
#df_taxes <- read_csv ('14zpallagi.csv')
```

#```{r}
df_dep <- df_taxes %>% filter(STATE == 'PA')%>% select(zipcode, agi_stub, NUMDEP) %>% pivot_wider(names_from = c( "agi_stub"), values_from = c("NUMDEP"))
df_dep_fr<- rename(df_dep, c('fr_loc_dep_1' = '1', 'fr_loc_dep_2' = '2', 'fr_loc_dep_3' = '3', 'fr_loc_dep_4'= '4', 'fr_loc_dep_5'= '5', 'fr_loc_dep_6'= '6' ))
df_sci_dep <- merge(df_s, df_dep_fr, by.x = 'fr_loc', by.y = 'zipcode')
df_dep_user<- rename(df_dep, c('user_loc_dep_1' = '1', 'user_loc_dep_2' = '2', 'user_loc_dep_3' = '3', 'user_loc_dep_4'= '4', 'user_loc_dep_5'= '5', 'user_loc_dep_6'= '6'))
df_sci_dep <- merge(df_sci_dep, df_dep_user, by.x = 'user_loc', by.y = 'zipcode')
#```

#```{r}
df_sci_dep <- df_sci_dep %>% mutate('diff_dep_1' = user_loc_dep_1 - fr_loc_dep_1)%>% mutate('diff_dep_2' = user_loc_dep_2 - fr_loc_dep_2)%>% mutate('diff_dep_3' = user_loc_dep_3 - fr_loc_dep_3)%>% mutate('diff_dep_4' = user_loc_dep_4 - fr_loc_dep_4)%>% mutate('diff_dep_5' = user_loc_dep_5 - fr_loc_dep_5)%>% mutate('diff_dep_6' = user_loc_dep_6 - fr_loc_dep_6)
df_sci_dep$distance_btwn <- df_sci_dep$distance$distance
df_sci_dep_simplified <- df_sci_dep %>% select(user_loc, fr_loc, wt_sci, distance_btwn, diff_dep_1, diff_dep_2, diff_dep_3, diff_dep_4, diff_dep_5, diff_dep_6)
cor(df_sci_dep_simplified[3:10])
data <- df_sci_dep_simplified[3:10] 
#```

#```{r}
tax_analysis <- function(variable){
  df_var <- df_taxes %>% filter(STATE == 'PA')%>% select(zipcode, agi_stub, variable) %>% pivot_wider(names_from = c( "agi_stub"), values_from = c(variable))
  df_var_fr<- rename(df_var, c('fr_loc_var_1' = '1','fr_loc_var_2' = '2', 'fr_loc_var_3' = '3', 'fr_loc_var_4'= '4', 'fr_loc_var_5'= '5', 'fr_loc_var_6'= '6' ))
  df_sci_var <- merge(df_s, df_var_fr, by.x = 'fr_loc', by.y = 'zipcode')
  df_var_user<- rename(df_var, c('user_loc_var_1' = '1', 'user_loc_var_2' = '2', 'user_loc_var_3' = '3', 'user_loc_var_4'= '4', 'user_loc_var_5'= '5', 'user_loc_var_6'= '6'))
  df_sci_var <- merge(df_sci_var, df_var_user, by.x = 'user_loc', by.y = 'zipcode')
  
  df_sci_var <- df_sci_var %>% mutate('diff_var_1' = user_loc_var_1 - fr_loc_var_1)%>% mutate('diff_var_2' = user_loc_var_2 - fr_loc_var_2)%>% mutate('diff_var_3' = user_loc_var_3 - fr_loc_var_3)%>% mutate('diff_var_4' = user_loc_var_4 - fr_loc_var_4)%>% mutate('diff_var_5' = user_loc_var_5 - fr_loc_var_5)%>% mutate('diff_var_6' = user_loc_var_6 - fr_loc_var_6)
  df_sci_var$distance_btwn <- df_sci_var$distance$distance
  return(df_sci_var)
}
df_n1<- tax_analysis('TCE')
#```
#```{r}
lm.fit <- lm(wt_sci~., data = data)
summary(lm.fit)
df_n1_simplified <- df_n1 %>% select(user_loc, fr_loc, wt_sci, distance_btwn, diff_var_1, diff_var_2, diff_var_3, diff_var_4, diff_var_5, diff_var_6)
cor(df_n1_simplified[3:10])
data <- df_n1_simplified[3:10] 
#```
Now we will import social determinant of health data from: https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html. We will use 2020 data since the SCI data is from 2020.
```{r}
library(readxl)
df_sdoh<- read_excel("SDOH_2020_ZIPCODE_1_0.xlsx", sheet = 'Data')
```
```{r}
df_sdoh_pa <- df_sdoh %>% filter(STATE == 'Pennsylvania')
```
After considering each possible predictor, we have selected predictors that seem like they might have helpful information regarding sci.
```{r}
#df_sdoh_pa_selected <- df_sdoh_pa %>% select(3, 36, 37, 38, 41, 48, 53, 55, 56, 62, 81, 85, 91, 92, 96, 103, 104, 120, 121, 125, 129, 134, 136, 162, 181, 193, 194, 199, 200, 201, 202, 203, 204, 209, 216, 227, 230, 231, 240, 241, 245, 247, 249, 250, 254, 264, 265, 279, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 299, 315, 318, 319)

df_sdoh_pa_selected <- df_sdoh_pa %>% select(3,10, 37, 41, 53, 55, 56, 62, 81, 85, 91, 92, 96, 103, 104, 120, 121, 134, 136, 162, 181, 216, 227, 230, 231, 247, 250, 254, 264, 265, 282, 283, 284, 289, 291, 292, 315, 318)
df_s <- df_s[order(as.numeric(rownames(df_s))),,drop=FALSE]
df_s$Count <- (1:nrow(df_s))
df_sdoh_sci_fr <- merge(df_s, df_sdoh_pa_selected, by.x = 'fr_loc', by.y = 'ZIPCODE')
df_sdoh_sci_fr <- df_sdoh_sci_fr[order(as.numeric(rownames(df_sdoh_sci_fr))),,drop=FALSE]
df_sdoh_sci_user <- merge(df_s, df_sdoh_pa_selected, by.x = 'user_loc', by.y = 'ZIPCODE')
df_sdoh_sci_user <- df_sdoh_sci_user[order(df_sdoh_sci_user$Count),,drop=FALSE]
df_full <- abs(df_sdoh_sci_fr[8:45] - df_sdoh_sci_user[ 8:45])

#Add respective data from sci data frame
df_full$distance <- df_s$distance$distance
#Take log of scaled_sci, from looking at the graphs of data and then the affects on analysis we decided to transform the scaled_sci
df_full$scaled_sci <- log(df_s$scaled_sci)
df_full$fr_loc <- df_s$fr_loc
df_full$user_loc <- df_s$user_loc
```


```{r}
#Clean the data a little: Drop NAs. Omit any rows with distance = 0. These are sci pairs within the same zipcode, and won't be able to be predicted from differences. This analysis will need to be done separately. 
df_full <- na.omit(df_full)

#Let's normalize the data
df_normalized <- df_full
for (i in 2:39){
  df_normalized[,i]<-((df_full[,i] - min(df_full[,i]))/((max(df_full[,i])-min(df_full[,i]))))
}

#Let's look a little at the correlations before digging in for further analysis
cor(df_normalized$scaled_sci, df_normalized[2:39])
data <- df_normalized[2:40] %>% filter(distance > 0)
```
There is a moderately strong negative correlation between distance and sci_scaled. None of the other variables have strong correlations but a few of them have some weak correlations (acs_pct_publ_transit_zc, acs_tot_pop_us_above1_zc, etc.)


```{r}
set.seed(2)

#Create a training and test set with an 80/20 split.
test_index <- sample(1:nrow(data), nrow(data)/5)
allnums<- (1:nrow(data))
train_index<- allnums[-test_index]
train <- data[train_index,]
test <- data[test_index,]
```
Now it is time to start building models.
Let's start by running multiple linear regression with all of the predictors. 
Since we are using MSE as a way to compare models, we will store all of the MSEs in one MSE list as we go.

```{r, warning = FALSE}
lm.fit <- lm(scaled_sci~., data = train)
predict <- predict.lm(lm.fit, newdata= test)

#Calculate MSE for this model and add to list
MSE.Full.MultiLin <- mean((test$scaled_sci - predict)^2)
MSE.Full.MultiLin

#Create empty MSE vector and empty vector names for the MSE
MSE_All <- c(MSE.Full.MultiLin)
MSE_Name <- c('Full MLR')
```
Full linear regression leads to an MSE of 1.068526.

Now we will try forward selection to find the best multiple linear regression model.
```{r, warning = FALSE}
#Fit Models
regfit.fwd <- regsubsets (scaled_sci ~ ., data = train, nvmax = 38 , method = "forward")
test.mat <- model.matrix(scaled_sci ~., data = test)

#Calculate MSE for each model size
val.errors <- rep (NA , 38)
for (i in 1:38) {
  coefi <- coef ( regfit.fwd , id = i)
  pred <- test.mat [, names ( coefi )] %*% coefi
  val.errors [ i] <- mean (( test$scaled_sci - pred ) ^2)
}
val.errors

#Find the model with the smallest MSE
which.min(val.errors)
MSE.Forward <- val.errors[which.min(val.errors)]
MSE.Forward

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.Forward)
MSE_Name <- c(MSE_Name, 'Fwd Select')
```
35 predictors minimize the MSE to 1.06489

Let's look more closely at this model:

```{r}
fwd_select_data <- subset(data, select = -c(ACS_PCT_BLACK_ZC, ACS_PER_CAPITA_INC_ZC, ACS_PCT_COLLEGE_ASSOCIATE_DGR_ZC))
fwd_train <- fwd_select_data[train_index,]
fwd_test <- fwd_select_data[test_index,]
fwd.fit <- lm(scaled_sci~. , data = fwd_train)
predict <- predict.lm(fwd.fit, newdata= fwd_test)
summary(fwd.fit)
```
The adjusted R-squared is .6144. In context, this tells us we are able to explain about 61% of the variability of log(scaled_sci) with our predictors. 
Next we will run Ridge Regression to see if we can decrease the MSE. Looking at the estimates of the coefficients, distance seems by far to explain the greatest variability. The other variables contributing most to the model are ACS_PCT_WHITE_ZC, ACS_TOT_POP_US_ABOVE1_ZC, and ACS_PCT_PUBL_TRANSIT_ZC.

```{r}
#Format data and grid to prep for Ridge Regression
x <- model.matrix(scaled_sci~., data)[,-1]
y <- data$scaled_sci
y.test <-y[test_index]
grid <- 10^seq(10, -2, length = 100)

#Fit the Model
ridge.mod <- glmnet(x[train_index,], y[train_index], alpha = 0, lamda = grid, thresh = 1e-12)

set.seed(30)

#Use Cross validation to find best value of lambda
cv.out <- cv.glmnet(x[train_index,], y[train_index], alpha = 0)
plot(cv.out)
bestlam <- cv.out$lambda.min
bestlam

#Fit predictions with best value of lambda
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test_index, ])

#Calculate MSE
MSE.ridge <- mean((ridge.pred - y.test)^2)
MSE.ridge

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.ridge)
MSE_Name <- c(MSE_Name, 'Ridge')
```
The ridge MSE is 1.076508, which is actually slightly higher than our forward selection model.

Next we will run Lasso
```{r}
#Build the model
lasso.mod <- glmnet(x[train_index, ], y[train_index], alpha =1, lamda = grid)
plot(lasso.mod)
set.seed(52)

#Use cross validation to select best value of lamda
cv.out <- cv.glmnet(x[train_index, ], y[train_index], alpha = 1)
plot(cv.out)
bestlam <- cv.out$lambda.min 

#Fit predictions for test set using best value of lambda
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test_index, ])

#Calculate MSE
MSE.lasso <- mean((lasso.pred - y.test)^2)

#Look at variables of interest
out <-  glmnet( x, y, alpha = 1, lamda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:18,]
lasso.coef
MSE.lasso

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.lasso)
MSE_Name <- c(MSE_Name, 'Lasso')
```
The MSE with Lasso is slightly higher than the MSE we got from forward selection: 1.06489.

Next we will create a tree
```{r, warning = FALSE}
set.seed(77)
#Build and plot the tree
data_tree <- df_full[2:40] %>% filter(distance > 0)
train_tree <- data_tree[train_index,]
test_tree <- data_tree[test_index,]
tree <- tree( scaled_sci ~ ., data_tree, subset = train_index )
plot(tree)
summary(tree)
text(tree, pretty = 0, cex = .5)

#Make predictions and calculate MSE
tree.pred <- predict(tree, test_tree)
MSE.tree <- mean((tree.pred - test_tree$scaled_sci)^2)
MSE.tree

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.tree)
MSE_Name <- c(MSE_Name, 'Full Tree')
```
The MSE of the full tree is .8495738, the lowest MSE yet.

We are able to improve the model from before, just using distance and ACS_PCT_PUBL_TRANSIT_ZC. 

Let's use cross validation to see if we should prune the tree
```{r, warning = FALSE}
#Run cross validation and plot to see which size is the best
cv.tree <- cv.tree(tree)
plot(cv.tree$size, cv.tree$dev, type = 'b')
```
It does not seem as though pruning the tree will improve the model, so we can stick with what we have so far. 

Although we have found the decision tree works bests, let's look at a graph of all the MSEs of all of the models we tried.
```{r}
plot(MSE_All)
color <- c(rep('Red', 4), 'Green')
MSE_All
barplot(MSE_All, names.arg = MSE_Name, las = 2, cex.names = .7, col = color, ylab = 'MSE', xlab = 'Model' )
```
We also need to investigate what causes variability in sci scores within one zip code.
```{r}
df_same_zip <- df_s %>% filter(distance$distance == 0)
df_same_zip_merged <- merge(df_same_zip, df_sdoh_pa_selected, by.x = 'user_loc', by.y = 'ZIPCODE')
same_zip_data <- subset(df_same_zip_merged, select = -c(user_loc, fr_loc, distance, Count, probabilites, wt, wt_sci))
same_zip_data <- na.omit(same_zip_data)

#Let's normalize the data
df_normalized_same <- same_zip_data
for (i in 2:38){
  df_normalized_same[,i]<-((same_zip_data[,i] - min(same_zip_data[,i]))/((max(same_zip_data[,i])-min(same_zip_data[,i]))))
}
same_zip_data <- df_normalized_same
cor(same_zip_data$scaled_sci, same_zip_data[2:38])
```
From the below plot, we can see that it might be of worth to take the log of sci. 

```{r}
plot(same_zip_data$scaled_sci, same_zip_data$ACS_PCT_HH_SMARTPHONE_ZC)
same_zip_data$scaled_sci <- log(same_zip_data$scaled_sci)

plot(same_zip_data$scaled_sci, same_zip_data$ACS_PCT_HH_SMARTPHONE_ZC)
cor(same_zip_data$scaled_sci, same_zip_data[2:38])
```
Most of the correlations have increased, which should help us with our model. 

```{r}
set.seed(18)

#Create a training and test set with an 80/20 split.
test_index_same <- sample(1:nrow(same_zip_data), nrow(same_zip_data)/5)
allnums<- (1:nrow(same_zip_data))
train_index_same<- allnums[-test_index_same]
train_same <- same_zip_data[train_index_same,]
test_same <- same_zip_data[test_index_same,]
```

Now it is time to start building models.
Let's start by running multiple linear regression with all of the predictors. 
Since we are using MSE as a way to compare models, we will store all of the MSEs in one MSE list as we go.

```{r, warning = FALSE}
lm.fit_same <- lm(scaled_sci~., data = train_same)
predict <- predict.lm(lm.fit_same, newdata= test_same)

#Calculate MSE for this model and add to list
MSE.Full.MultiLin_same <- mean((test_same$scaled_sci - predict)^2)
MSE.Full.MultiLin_same

#Create empty MSE vector and empty vector names for the MSE
MSE_All_Same <- c(MSE.Full.MultiLin_same)
MSE_Name_Same <- c('Full MLR')
```
Our MSE for the multiple linear regression is .362492

Now we will try forward selection to find the best multiple linear regression model.
```{r, warning = FALSE}
#Fit Models
regfit.fwd_same <- regsubsets (scaled_sci ~ ., data = train_same, nvmax = 37 , method = "forward")
test.mat_same <- model.matrix(scaled_sci ~., data = test_same)
regfit.fwd_same

#Calculate MSE for each model size
val.errors_same <- rep (NA , 37)
for (i in 1:37) {
  coefi <- coef ( regfit.fwd_same , id = i)
  pred <- test.mat_same [, names ( coefi )] %*% coefi
  val.errors_same [ i] <- mean (( test_same$scaled_sci - pred ) ^2)
}
val.errors_same

#Find the model with the smallest MSE
which.min(val.errors_same)
MSE.Forward_same <- val.errors_same[which.min(val.errors_same)]
MSE.Forward_same

#Add MSE and Name to lists
MSE_All_Same <- c(MSE_All_Same, MSE.Forward_same)
MSE_Name_Same <- c(MSE_Name_Same, 'Fwd Select')
```
9 predictors minimize our MSE to .3447035.

Let's look more closely at this model
```{r}
fwd_select_data_same <- subset(same_zip_data, select = c(scaled_sci, ACS_TOT_POP_US_ABOVE1_ZC, ACS_PCT_FOREIGN_BORN_ZC, ACS_PCT_HISPANIC_ZC, ACS_PCT_WHITE_ZC, ACS_PCT_HH_SMARTPHONE_ZC, ACS_PCT_HH_CELLULAR_ZC, ACS_PCT_HH_FOOD_STMP_BLW_POV_ZC, ACS_PCT_BACHELOR_DGR_ZC, ACS_PCT_HU_NO_VEH_ZC))
fwd_train_same <- fwd_select_data_same[train_index_same,]
fwd_test_same <- fwd_select_data_same[test_index_same,]
fwd.fit_same <- lm(scaled_sci~. , data = fwd_train_same)
predict <- predict.lm(fwd.fit_same, newdata= fwd_test_same)
summary(fwd.fit_same)
```
With this model we have a multiple r squared of .7539, meaning we are able to explain about 75% of the variability in log(scaled_sci) with the 9 above predictors.ACS_TOT_POP_US_ABOVE1_ZC seems to have the highest impact on our model and ACS_PCT_BACHELOR_DGR_ZC having the second highest impact. ACS_PCT_HH_SMARTPHONE_ZC seems to contribute the least out of all the variables in the model, while the other 6 variables seem to contribute at similar rates.

Next we will run Ridge Regression on the same zipcodes to see if we can decrease the MSE
```{r}
#Format data and grid to prep for Ridge Regression
x <- model.matrix(scaled_sci~., same_zip_data)[,-1]
y <- same_zip_data$scaled_sci
y.test <-y[test_index_same]
grid <- 10^seq(10, -2, length = 100)

#Fit the Model
ridge.mod_same <- glmnet(x[train_index_same,], y[train_index_same], alpha = 0, lamda = grid, thresh = 1e-12)

set.seed(15)

#Use Cross validation to find best value of lambda
cv.out_same <- cv.glmnet(x[train_index_same,], y[train_index_same], alpha = 0)
plot(cv.out_same)
bestlam_same <- cv.out_same$lambda.min
bestlam_same

#Fit predictions with best value of lambda
ridge.pred_same <- predict(ridge.mod_same, s = bestlam_same, newx = x[test_index_same, ])

#Calculate MSE
MSE.ridge_same <- mean((ridge.pred_same - y.test)^2)
MSE.ridge_same

#Add MSE and Name to lists
MSE_All_Same <- c(MSE_All_Same, MSE.ridge_same)
MSE_Name_Same <- c(MSE_Name_Same, 'Ridge')
```
The ridge MSE is .3546, which is actually slightly higher than our forward selection model.

Next we will run Lasso
```{r}
#Build the model
lasso.mod_same <- glmnet(x[train_index_same, ], y[train_index_same], alpha =1, lamda = grid)
plot(lasso.mod_same)
set.seed(117)

#Use cross validation to select best value of lamda
cv.out_same <- cv.glmnet(x[train_index_same, ], y[train_index_same], alpha = 1)
plot(cv.out_same)
bestlam_same <- cv.out_same$lambda.min 

#Fit predictions for test set using best value of lambda
lasso.pred_same <- predict(lasso.mod_same, s = bestlam_same, newx = x[test_index_same, ])

#Calculate MSE
MSE.lasso_same <- mean((lasso.pred_same - y.test)^2)

#Look at variables of interest
out <-  glmnet( x, y, alpha = 1, lamda = grid)
lasso.coef <- predict(out, type = 'coefficients', s = bestlam)[1:18,]
lasso.coef
MSE.lasso_same

#Add MSE and Name to lists
MSE_All_Same <- c(MSE_All_Same, MSE.lasso_same)
MSE_Name_Same <- c(MSE_Name_Same, 'Lasso')
```
The MSE with Lasso is slightly lower than the MSE we got from forward selection: .342968 vs .3447035.

Next we will create a tree
```{r, warning = FALSE}
set.seed(77)
#Build and plot the tree
tree_same <- tree( scaled_sci ~ ., same_zip_data, subset = train_index_same )
plot(tree_same)
summary(tree_same)
text(tree_same, pretty = 0, cex = .5)

#Make predictions and calculate MSE
tree.pred_same <- predict(tree_same, test_same)
MSE.tree_same <- mean((tree.pred_same - test_same$scaled_sci)^2)
MSE.tree_same
```

```{r}
#Add MSE and Name to lists
MSE_All_Same <- c(MSE_All_Same, MSE.tree)
MSE_Name_Same <- c(MSE_Name_Same, 'Full Tree')
```
The MSE is higher for the decision tree than it is for Lasso or forward selection.

Although we have found the decision tree works bests, let's look at a graph of all the MSEs of all of the models we tried.
```{r}
plot(MSE_All_Same)
color <- c('Red', 'Green', rep('Red', 3))
barplot(MSE_All_Same, names.arg = MSE_Name_Same, las = 2, cex.names = .7, col = color, ylab = 'MSE within Zipcodes', xlab = 'Model' )
```