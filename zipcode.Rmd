---
title: "Social Proximity Zipcode"
author: 
date: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Social Networks can be a significant factor in understanding the dynamics of various aspects of different fields such as social,economical and public health. In order to substantiate the intensity of social network between individuals Bailey et al proposed a relative frequency of Friendship links between different geographical unit called as Social Connectedness Index. Due to the nature of its relative frequency a comparison between the intensity of social ties across different locations can now be possible. Further Kulcher et al proposed another measure understanding the social influence of alters on ego called social proximity. This markdown file is a guide for other researchers who would want to make use of Social Proximity in there research.

```{r}
library(tidyverse)
library(igraph)
library(glmnet)
library(pls)
library(leaps)
library(class)
```

We create the social proximity values for each zipcode beginning with a 1. We then drop zipcodes that fall outside of Pennsylvania. As a first step we want to convert the SCI values into relative probabilities so we divide the SCI by the highest frequency of SCI and create a data frame with user location fr_locations and their probability values.

```{r}
df_0 <- read_tsv ('zcta_zcta_shard1.tsv')
df_0 <- df_0 %>% dplyr::mutate(probabilites=scaled_sci/(1000000000)) 
df_1 <- df_0 %>% dplyr::filter(user_loc %in% 15001:19612 & fr_loc %in% 15001:19612)
df_1 <- unique(df_1)
df_1 <- df_1 %>% distinct(probabilites,.keep_all = TRUE) ##single repeated pairwise comparisons for sci
user_loc <- df_1 %>% distinct(user_loc)
```

```{r}
library(zipcodeR)
df_1 <- df_1 %>% mutate('distance' = zip_distance(df_1$user_loc, df_1$fr_loc))

```


Generating weights for  1256 zipcodes and merging it in a data frame.
```{r}
dim(user_loc)
fr_loc <- df_1 %>% distinct(fr_loc)
wt <- rnorm(1256,0.13,1)
q_i <- data.frame(fr_loc,wt)
```

Further to create a weighted probabilities scaled by SCI and the weights of friend location we create a new data frame and mutate it with weighted SCI. 
```{r}
df_s <- merge(df_1,q_i,by="fr_loc")
df_s <- df_s %>% mutate(wt_sci=probabilites*wt) %>% filter(scaled_sci != 1)
```
Now let's do some plotting of distance with wt_sci. 
```{r}
plot(df_s$distance$distance, df_s$wt_sci)
plot(df_s$distance$distance, df_s$wt_sci, xlim = c(0, 30))
```
```{r}
# Download USA Income Tax Data by Zip code here: https://www.kaggle.com/datasets/wpncrh/zip-code-income-tax-data-2014?resource=download
#df_taxes <- read_csv ('14zpallagi.csv')
```

#```{r}
df_dep <- df_taxes %>% filter(STATE == 'PA')%>% select(zipcode, agi_stub, NUMDEP) %>% pivot_wider(names_from = c( "agi_stub"), values_from = c("NUMDEP"))
df_dep_fr<- rename(df_dep, c('fr_loc_dep_1' = '1', 'fr_loc_dep_2' = '2', 'fr_loc_dep_3' = '3', 'fr_loc_dep_4'= '4', 'fr_loc_dep_5'= '5', 'fr_loc_dep_6'= '6' ))
df_sci_dep <- merge(df_s, df_dep_fr, by.x = 'fr_loc', by.y = 'zipcode')
df_dep_user<- rename(df_dep, c('user_loc_dep_1' = '1', 'user_loc_dep_2' = '2', 'user_loc_dep_3' = '3', 'user_loc_dep_4'= '4', 'user_loc_dep_5'= '5', 'user_loc_dep_6'= '6'))
df_sci_dep <- merge(df_sci_dep, df_dep_user, by.x = 'user_loc', by.y = 'zipcode')
#```

#```{r}
df_sci_dep <- df_sci_dep %>% mutate('diff_dep_1' = user_loc_dep_1 - fr_loc_dep_1)%>% mutate('diff_dep_2' = user_loc_dep_2 - fr_loc_dep_2)%>% mutate('diff_dep_3' = user_loc_dep_3 - fr_loc_dep_3)%>% mutate('diff_dep_4' = user_loc_dep_4 - fr_loc_dep_4)%>% mutate('diff_dep_5' = user_loc_dep_5 - fr_loc_dep_5)%>% mutate('diff_dep_6' = user_loc_dep_6 - fr_loc_dep_6)
df_sci_dep$distance_btwn <- df_sci_dep$distance$distance
df_sci_dep_simplified <- df_sci_dep %>% select(user_loc, fr_loc, wt_sci, distance_btwn, diff_dep_1, diff_dep_2, diff_dep_3, diff_dep_4, diff_dep_5, diff_dep_6)
cor(df_sci_dep_simplified[3:10])
data <- df_sci_dep_simplified[3:10] 
#```

#```{r}
tax_analysis <- function(variable){
  df_var <- df_taxes %>% filter(STATE == 'PA')%>% select(zipcode, agi_stub, variable) %>% pivot_wider(names_from = c( "agi_stub"), values_from = c(variable))
  df_var_fr<- rename(df_var, c('fr_loc_var_1' = '1','fr_loc_var_2' = '2', 'fr_loc_var_3' = '3', 'fr_loc_var_4'= '4', 'fr_loc_var_5'= '5', 'fr_loc_var_6'= '6' ))
  df_sci_var <- merge(df_s, df_var_fr, by.x = 'fr_loc', by.y = 'zipcode')
  df_var_user<- rename(df_var, c('user_loc_var_1' = '1', 'user_loc_var_2' = '2', 'user_loc_var_3' = '3', 'user_loc_var_4'= '4', 'user_loc_var_5'= '5', 'user_loc_var_6'= '6'))
  df_sci_var <- merge(df_sci_var, df_var_user, by.x = 'user_loc', by.y = 'zipcode')
  
  df_sci_var <- df_sci_var %>% mutate('diff_var_1' = user_loc_var_1 - fr_loc_var_1)%>% mutate('diff_var_2' = user_loc_var_2 - fr_loc_var_2)%>% mutate('diff_var_3' = user_loc_var_3 - fr_loc_var_3)%>% mutate('diff_var_4' = user_loc_var_4 - fr_loc_var_4)%>% mutate('diff_var_5' = user_loc_var_5 - fr_loc_var_5)%>% mutate('diff_var_6' = user_loc_var_6 - fr_loc_var_6)
  df_sci_var$distance_btwn <- df_sci_var$distance$distance
  return(df_sci_var)
}
df_n1<- tax_analysis('TCE')
#```
#```{r}
lm.fit <- lm(wt_sci~., data = data)
summary(lm.fit)
df_n1_simplified <- df_n1 %>% select(user_loc, fr_loc, wt_sci, distance_btwn, diff_var_1, diff_var_2, diff_var_3, diff_var_4, diff_var_5, diff_var_6)
cor(df_n1_simplified[3:10])
data <- df_n1_simplified[3:10] 
#```
Now we will import social determinant of health data from: https://www.ahrq.gov/sdoh/data-analytics/sdoh-data.html. We will use 2020 data since the SCI data is from 2020.
```{r}
library(readxl)
df_sdoh<- read_excel("SDOH_2020_ZIPCODE_1_0.xlsx", sheet = 'Data')
```
```{r}
df_sdoh_pa <- df_sdoh %>% filter(STATE == 'Pennsylvania')
```
After considering each possible predictor, we have selected predictors that seem like they might have helpful information regarding sci.
```{r}
#df_sdoh_pa_selected <- df_sdoh_pa %>% select(3, 36, 37, 38, 41, 48, 53, 55, 56, 62, 81, 85, 91, 92, 96, 103, 104, 120, 121, 125, 129, 134, 136, 162, 181, 193, 194, 199, 200, 201, 202, 203, 204, 209, 216, 227, 230, 231, 240, 241, 245, 247, 249, 250, 254, 264, 265, 279, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 299, 315, 318, 319)

df_sdoh_pa_selected <- df_sdoh_pa %>% select(3,10, 37, 41, 53, 55, 56, 62, 81, 85, 91, 92, 96, 103, 104, 120, 121, 134, 136, 162, 181, 216, 227, 230, 231, 247, 250, 254, 264, 265, 282, 283, 284, 289, 291, 292, 315, 318)
df_s <- df_s[order(as.numeric(rownames(df_s))),,drop=FALSE]
df_s$Count <- (1:nrow(df_s))
df_sdoh_sci_fr <- merge(df_s, df_sdoh_pa_selected, by.x = 'fr_loc', by.y = 'ZIPCODE')
df_sdoh_sci_fr <- df_sdoh_sci_fr[order(as.numeric(rownames(df_sdoh_sci_fr))),,drop=FALSE]
df_sdoh_sci_user <- merge(df_s, df_sdoh_pa_selected, by.x = 'user_loc', by.y = 'ZIPCODE')
df_sdoh_sci_user <- df_sdoh_sci_user[order(df_sdoh_sci_user$Count),,drop=FALSE]
df_full <- abs(df_sdoh_sci_fr[8:45] - df_sdoh_sci_user[ 8:45])

#Add respective data from sci data frame
df_full$distance <- df_s$distance$distance
#Take log of scaled_sci, from looking at the graphs of data and then the affects on analysis we decided to transform the scaled_sci
df_full$scaled_sci <- log(df_s$scaled_sci)
df_full$fr_loc <- df_s$fr_loc
df_full$user_loc <- df_s$user_loc
```

```{r}
#Clean the data a little: Drop NAs. Omit any rows with distance = 0. These are sci pairs within the same zipcode, and won't be able to be predicted from differences. This analysis will need to be done separately. 
df_full <- na.omit(df_full)
data <- df_full[2:40] %>% filter(distance > 0)


#Let's look a little at the correlations before digging in for further analysis
cor(df_full$scaled_sci, df_full[2:39])
```
There is a moderately strong negative correlation between distance and sci_scaled.None of the other variables have strong correlations but a few of them have some weak corelations (Cen_popdensity_zc, acs_tot_pop_us_above1_zc, etc.)


```{r}
set.seed(2)

#Create a training and test set with an 80/20 split.
test_index <- sample(1:nrow(data), nrow(data)/5)
allnums<- (1:nrow(data))
train_index<- allnums[-test_index]
train <- data[train_index,]
test <- data[test_index,]
```
Now it is time to start building models.
Let's start by running multiple linear regression with all of the predictors. 
Since we are using MSE as a way to compare models, we will store all of the MSEs in one MSE list as we go.

```{r, warning = FALSE}
lm.fit <- lm(scaled_sci~., data = train)
predict <- predict.lm(lm.fit, newdata= test)

#Calculate MSE for this model and add to list
MSE.Full.MultiLin <- mean((test$scaled_sci - predict)^2)
MSE.Full.MultiLin

#Create empty MSE vector and empty vector names for the MSE
MSE_All <- c(MSE.Full.MultiLin)
MSE_Name <- c('Full MLR')
```

Now we will try forward selection to find the best multiple linear regression model.
```{r, warning = FALSE}
#Fit Models
regfit.fwd <- regsubsets (scaled_sci ~ ., data = train, nvmax = 38 , method = "forward")
test.mat <- model.matrix(scaled_sci ~., data = test)
regfit.fwd

#Calculate MSE for each model size
val.errors <- rep (NA , 38)
for (i in 1:38) {
  coefi <- coef ( regfit.fwd , id = i)
  pred <- test.mat [, names ( coefi )] %*% coefi
  val.errors [ i] <- mean (( test$scaled_sci - pred ) ^2)
}
val.errors

#Find the model with the smallest MSE
which.min(val.errors)
MSE.Forward <- val.errors[which.min(val.errors)]
MSE.Forward

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.Forward)
MSE_Name <- c(MSE_Name, 'Fwd Select')
```
35 predictors minimize the MSE to 1.06849.

Let's look more closely at this model
```{r}
fwd_select_data <- subset(data, select = -c(ACS_PCT_ASIAN_ZC, ACS_PCT_HH_NO_INTERNET_ZC, ACS_PCT_COLLEGE_ASSOCIATE_DGR_ZC))
fwd_train <- fwd_select_data[train_index,]
fwd_test <- fwd_select_data[test_index,]
fwd.fit <- lm(scaled_sci~. , data = fwd_train)
predict <- predict.lm(fwd.fit, newdata= fwd_test)
summary(fwd.fit)
```
We also need to investigate what causes variability in sci scores within one zip code.
```{r}
df_same_zip <- df_s %>% filter(distance$distance == 0)
df_same_zip_merged <- merge(df_same_zip, df_sdoh_pa_selected, by.x = 'user_loc', by.y = 'ZIPCODE')
same_zip_data <- subset(df_same_zip_merged, select = -c(user_loc, fr_loc, distance, Count, probabilites, wt, wt_sci))
same_zip_data <- na.omit(same_zip_data)
cor(same_zip_data$scaled_sci, same_zip_data[2:38])
```
From the below plot, we can see that it might be of worth to take the log of sci. 
```{r}
plot(same_zip_data$scaled_sci, same_zip_data$ACS_PCT_HH_SMARTPHONE_ZC)
same_zip_data$scaled_sci <- log(same_zip_data$scaled_sci)
plot(same_zip_data$scaled_sci, same_zip_data$ACS_PCT_HH_SMARTPHONE_ZC)
cor(same_zip_data$scaled_sci, same_zip_data[2:38])
```
Most of the correlations have increased, which should help us with our model. 

```{r}
set.seed(18)

#Create a training and test set with an 80/20 split.
test_index_same <- sample(1:nrow(same_zip_data), nrow(same_zip_data)/5)
allnums<- (1:nrow(same_zip_data))
train_index_same<- allnums[-test_index_same]
train_same <- same_zip_data[train_index_same,]
test_same <- same_zip_data[test_index_same,]
```

Now it is time to start building models.
Let's start by running multiple linear regression with all of the predictors. 
Since we are using MSE as a way to compare models, we will store all of the MSEs in one MSE list as we go.

```{r, warning = FALSE}
lm.fit_same <- lm(scaled_sci~., data = train_same)
predict <- predict.lm(lm.fit_same, newdata= test_same)

#Calculate MSE for this model and add to list
MSE.Full.MultiLin_same <- mean((test_same$scaled_sci - predict)^2)
MSE.Full.MultiLin_same

#Create empty MSE vector and empty vector names for the MSE
MSE_All <- c(MSE.Full.MultiLin_same)
MSE_Name <- c('Full MLR Same Zip')
```

Now we will try forward selection to find the best multiple linear regression model.
```{r, warning = FALSE}
#Fit Models
regfit.fwd_same <- regsubsets (scaled_sci ~ ., data = train_same, nvmax = 37 , method = "forward")
test.mat_same <- model.matrix(scaled_sci ~., data = test_same)
regfit.fwd_same

#Calculate MSE for each model size
val.errors_same <- rep (NA , 37)
for (i in 1:37) {
  coefi <- coef ( regfit.fwd_same , id = i)
  pred <- test.mat_same [, names ( coefi )] %*% coefi
  val.errors_same [ i] <- mean (( test_same$scaled_sci - pred ) ^2)
}
val.errors_same

#Find the model with the smallest MSE
which.min(val.errors_same)
MSE.Forward_same <- val.errors_same[which.min(val.errors_same)]
MSE.Forward_same

#Add MSE and Name to lists
MSE_All <- c(MSE_All, MSE.Forward_same)
MSE_Name <- c(MSE_Name, 'Fwd Select Same')
```
9 predictors minimize our MSE to .3447035.

Let's look more closely at this model
```{r}
fwd_select_data_same <- subset(same_zip_data, select = c(scaled_sci, ACS_TOT_POP_US_ABOVE1_ZC, ACS_PCT_FOREIGN_BORN_ZC, ACS_PCT_HISPANIC_ZC, ACS_PCT_WHITE_ZC, ACS_PCT_HH_SMARTPHONE_ZC, ACS_PCT_HH_CELLULAR_ZC, ACS_PCT_HH_FOOD_STMP_BLW_POV_ZC, ACS_PCT_BACHELOR_DGR_ZC, ACS_PCT_HU_NO_VEH_ZC))
fwd_train_same <- fwd_select_data_same[train_index_same,]
fwd_test_same <- fwd_select_data_same[test_index_same,]
fwd.fit_same <- lm(scaled_sci~. , data = fwd_train_same)
predict <- predict.lm(fwd.fit_same, newdata= fwd_test_same)
summary(fwd.fit_same)
```
With this model we have a multiple r squared of .7539, meaning we are able to explain about 75% of the variability in log(scaled_sci) with the 9 above predictors.